{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda,Dropout,Cropping2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to locate path and Read image \n",
    "def read_image(source_path):\n",
    "    filename = source_path.split('/')[-1]\n",
    "    current_path = '/home/yue/CarND-Behavioral-Cloning/Training_data3/IMG/' + filename\n",
    "    image = cv2.imread(current_path)\n",
    "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return converted_image\n",
    "\n",
    "\n",
    "# Open csv file\n",
    "lines = []\n",
    "with open('/home/yue/CarND-Behavioral-Cloning/Training_data3/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_image_paths = []\n",
    "center_steers = []\n",
    "\n",
    "left_image_paths = []\n",
    "left_steers = []\n",
    "\n",
    "right_image_paths = []\n",
    "right_steers = []\n",
    "\n",
    "image_paths = []\n",
    "steers = [] \n",
    "\n",
    "for line in lines:\n",
    "    #center\n",
    "    center_image_paths.append(line[0])\n",
    "    center_steers.append(float(line[3]))\n",
    "    \n",
    "    #left\n",
    "    left_image_paths.append(line[1])\n",
    "    left_steers.append(float(line[3])+0.25)\n",
    "    \n",
    "    #right\n",
    "    right_image_paths.append(line[2])\n",
    "    right_steers.append(float(line[3])-0.25)\n",
    "    \n",
    "    image_paths.append(line[0])\n",
    "    steers.append(float(line[3]))\n",
    "    \n",
    "    #left\n",
    "    image_paths.append(line[1])\n",
    "    steers.append(float(line[3])+0.25)\n",
    "    \n",
    "    #right\n",
    "    image_paths.append(line[2])\n",
    "    steers.append(float(line[3])-0.25)\n",
    "    \n",
    "\n",
    "center_image_paths = np.array(center_image_paths)\n",
    "center_steers = np.array(center_steers)\n",
    "\n",
    "left_image_paths = np.array(left_image_paths)\n",
    "left_steers = np.array(left_steers)\n",
    "\n",
    "right_image_paths = np.array(right_image_paths)\n",
    "right_steers = np.array(right_steers)\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "steers = np.array(steers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504\n",
      "10504\n"
     ]
    }
   ],
   "source": [
    "def data_distribution_normalize(image_paths, steers): \n",
    "    num_bins = 50\n",
    "    n, bins = np.histogram(steers, num_bins)\n",
    "    \n",
    "    keep_probs = []\n",
    "    target = len(steers)/(num_bins)\n",
    "\n",
    "    remove_list = []\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        if n[i] < target:\n",
    "            keep_probs.append(1.)\n",
    "        else:\n",
    "            keep_probs.append(target/n[i])\n",
    "\n",
    "    for i in range(len(steers)):\n",
    "        for j in range(num_bins):\n",
    "            if steers[i] > bins[j] and steers[i] <= bins[j+1]:\n",
    "                if np.random.rand() > keep_probs[j]:\n",
    "                    remove_list.append(i)\n",
    "                    \n",
    "    image_paths = np.delete(image_paths, remove_list)\n",
    "    steers = np.delete(steers, remove_list)\n",
    "    \n",
    "    return image_paths, steers\n",
    "\n",
    "\n",
    "'''\n",
    "center_image_paths, center_steers = data_distribution_normalize(center_image_paths, center_steers)\n",
    "left_image_paths, left_steers = data_distribution_normalize(left_image_paths, left_steers)\n",
    "right_image_paths, right_steers = data_distribution_normalize(right_image_paths, right_steers)\n",
    "#plt.hist(right_steers, 50)\n",
    "dist = []\n",
    "dist.append(len(center_image_paths))\n",
    "dist.append(len(left_image_paths))\n",
    "dist.append(len(right_image_paths))\n",
    "for num in dist:\n",
    "    print(num)\n",
    "\n",
    "\n",
    "image_paths = np.concatenate((center_image_paths,left_image_paths,right_image_paths))\n",
    "steers = np.concatenate((center_steers,left_steers,right_steers))\n",
    "print(len(image_paths))\n",
    "'''    \n",
    "new_image_paths, new_steers = data_distribution_normalize(image_paths, steers)\n",
    "print(len(new_image_paths))\n",
    "print(len(new_steers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8403\n",
      "2101\n"
     ]
    }
   ],
   "source": [
    "# split dataset to 80% training data, 20% validation data\n",
    "train_samples_paths, validation_samples_paths, train_steers, validation_steers = train_test_split(new_image_paths, new_steers,test_size = 0.2)\n",
    "print(len(train_samples_paths))\n",
    "\n",
    "print(len(validation_samples_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_image(image, steer,trans_range):\n",
    "    rand_num = np.random.uniform(-trans_range,trans_range)\n",
    "    rand_num2 = np.random.uniform(-trans_range,trans_range)\n",
    "    rows, cols, ch = image.shape\n",
    "    h_shift = rand_num\n",
    "    trans_steer = steer + h_shift * 0.004\n",
    "    v_shift = rand_num2\n",
    "    M = np.float32([[1,0,h_shift],[0,1,v_shift]])\n",
    "    trans_image = cv2.warpAffine(image,M,(cols,rows))\n",
    "    \n",
    "    return trans_image, trans_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shadow(image):\n",
    "    new_img = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    new_img = np.array(new_img, dtype = np.float64)\n",
    "\n",
    "    h,w = new_img.shape[0:2]\n",
    "    mid = np.random.randint(0,w)\n",
    "\n",
    "\n",
    "    factor = np.random.uniform(0.6,0.8)\n",
    "    if np.random.rand() > .5:\n",
    "        new_img[:,0:mid,0] *= factor\n",
    "    else:\n",
    "        new_img[:,mid:w,0] *= factor\n",
    "\n",
    "    shadow_image = np.array(new_img, dtype = np.uint8)  \n",
    "    #shadow_image = cv2.cvtColor(shadow_image,cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    return shadow_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_image(image):\n",
    "    image_tmp = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    image_tmp = np.array(image_tmp, dtype = np.float64)\n",
    "    random_bright = .5+np.random.uniform()\n",
    "    image_tmp[:,:,0] = image_tmp[:,:,2]*random_bright\n",
    "    image_tmp[:,:,0][image_tmp[:,:,2]>255]  = 255\n",
    "    image_tmp = np.array(image_tmp, dtype = np.uint8)\n",
    "    #image_tmp = cv2.cvtColor(image_tmp,cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return image_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(image_paths,steers, aug_num):\n",
    "    new_images = []\n",
    "    new_measurements = []\n",
    "    for i in range(len(image_paths)):\n",
    "        image = read_image(image_paths[i])\n",
    "        steer = steers[i]\n",
    "        new_images.append(image)\n",
    "        new_measurements.append(steer)\n",
    "        flip_image = np.fliplr(image)\n",
    "        new_images.append(flip_image)\n",
    "        new_measurements.append(-steer)\n",
    "\n",
    "        for i in range(aug_num):\n",
    "            new_image = brightness_image(image)\n",
    "            new_image = random_shadow(new_image)\n",
    "            new_image, new_steer = translation_image(new_image, steer,30) \n",
    "            new_images.append(new_image)\n",
    "            new_measurements.append(new_steer)\n",
    "\n",
    "            new_flip_image = brightness_image(flip_image)\n",
    "            new_flip_image = random_shadow(new_flip_image)\n",
    "            new_flip_image, new_flip_steer = translation_image(new_flip_image, -steer,30) \n",
    "            new_images.append(new_flip_image)\n",
    "            new_measurements.append(new_flip_steer)\n",
    "    \n",
    "    return np.array(new_images), np.array(new_measurements)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with coroutine\n",
    "def train_generator(samples, steers, batch_size = 32):\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        # shuffle data\n",
    "        sklearn.utils.shuffle(samples,steers)\n",
    "        #Loop over batches\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            batch_steers = steers[offset:offset+batch_size]\n",
    "                    \n",
    "            images, measurements = data_augmentation(batch_samples, batch_steers,1)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(measurements)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with coroutine\n",
    "def valid_generator(samples, steers, batch_size = 32):\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        # shuffle data\n",
    "        sklearn.utils.shuffle(samples, steers)\n",
    "        #Loop over batches\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            batch_steers = steers[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            measurements = []\n",
    "            for i in range(len(batch_samples)):\n",
    "\n",
    "                images.append(read_image(batch_samples[i])) \n",
    "                \n",
    "                measurements.append(float(steers[i]))\n",
    "\n",
    "            X_valid = np.array(images)\n",
    "            y_valid = np.array(measurements)\n",
    "            yield sklearn.utils.shuffle(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "training_generator = train_generator(train_samples_paths, train_steers, batch_size=batch_size)\n",
    "validation_generator = valid_generator(validation_samples_paths, validation_steers, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), activation=\"relu\", strides=(2, 2))`\n",
      "  import sys\n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), activation=\"relu\", strides=(2, 2))`\n",
      "  \n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), activation=\"relu\", strides=(2, 2))`\n",
      "  if __name__ == '__main__':\n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/yue/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=2101, epochs=5, steps_per_epoch=33612)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   56/33612 [..............................] - ETA: 4:31:49 - loss: 0.0574"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a74a8355a0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m')`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0msignature\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m')`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m     \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n\u001b[0m\u001b[1;32m   2333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;31m# Arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Normalize the data\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "#Crop images to 80X320X3\n",
    "model.add(Cropping2D(cropping=((60,20),(0,0))))\n",
    "#Nvidia Network\n",
    "model.add(Convolution2D(24,5,5,subsample=(2,2),activation = 'relu'))\n",
    "model.add(Convolution2D(36,5,5,subsample=(2,2),activation = 'relu'))\n",
    "model.add(Convolution2D(48,3,3,subsample=(2,2),activation = 'relu'))\n",
    "model.add(Convolution2D(64,3,3,subsample=(1,1),activation = 'relu'))\n",
    "model.add(Convolution2D(64,3,3,subsample=(1,1),activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#Optimize MSE using ADAM optimizer\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "train_steps = np.ceil(len(train_samples_paths)*4/batch_size).astype(np.int32)\n",
    "validation_steps =np.ceil(len(validation_samples_paths)/batch_size).astype(np.int32)\n",
    "\n",
    "model.fit_generator(training_generator, samples_per_epoch=len(train_samples_paths)*4, validation_data=validation_generator, nb_val_samples=len(validation_samples_paths), nb_epoch = 5)\n",
    "\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
